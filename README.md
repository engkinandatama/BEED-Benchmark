# BEED Benchmark: A Modular Machine Learning Pipeline for EEG-based Epileptic Seizure Detection

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

> **BEED** (Biomedical EEG Epilepsy Detection) Benchmark is a comprehensive, modular implementation for benchmarking machine learning algorithms on EEG-based epileptic seizure detection datasets. This pipeline provides standardized evaluation protocols, reproducible results, and extensible architecture for epilepsy research.

## ğŸš€ Key Features

- **ğŸ”¬ Comprehensive Evaluation**: 10-fold stratified cross-validation with multiple metrics
- **ğŸ§  Multiple ML Algorithms**: Support for 9+ state-of-the-art machine learning models
- **âš¡ Hyperparameter Optimization**: Automated hyperparameter tuning with random search
- **ğŸ¯ Feature Selection**: Advanced feature selection techniques with stability analysis
- **ğŸ“Š Rich Visualizations**: Publication-ready plots and comprehensive result analysis
- **ğŸ”„ Reproducible**: Fixed random seeds and version-controlled dependencies
- **ğŸ’¾ Checkpointing**: Resume long-running experiments from interruptions
- **ğŸ“ˆ External Validation**: Cross-dataset validation capabilities
- **ğŸ Modern Python**: Type hints, dataclasses, and clean architecture

## ğŸ“‹ Table of Contents

- [Installation](#installation)
- [Quick Start](#quick-start)
- [Project Structure](#project-structure)
- [Usage Guide](#usage-guide)
- [Configuration](#configuration)
- [Supported Models](#supported-models)
- [Results & Evaluation](#results--evaluation)
- [Contributing](#contributing)
- [Citation](#citation)
- [License](#license)

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager
- (Optional) CUDA-compatible GPU for XGBoost acceleration

### Install from PyPI (Recommended)

```bash
pip install beed-benchmark
```

### Install from Source

```bash
git clone https://github.com/engkinandatama/beed-benchmark.git
cd beed-benchmark
pip install -r requirements.txt
pip install -e .
```

### Development Installation

```bash
git clone https://github.com/engkinandatama/beed-benchmark.git
cd beed-benchmark
pip install -r requirements-dev.txt
pip install -e .
pre-commit install
```

## ğŸš€ Quick Start

### Basic Usage

```python
from beed_benchmark import BEEDBenchmark
from beed_benchmark.config import PATHS, CONFIG

# Initialize benchmark
benchmark = BEEDBenchmark(
    data_path="path/to/your/eeg_data.csv",
    output_dir="results/"
)

# Run complete benchmark
results = benchmark.run_full_pipeline()

# View results
print(f"Best model: {results.best_model}")
print(f"Best F1-score: {results.best_score:.4f}")
```

### Command Line Interface

```bash
# Run basic benchmark
python -m beed_benchmark --data-path data/BEED_Data.csv --output-dir results/

# Run with hyperparameter tuning
python -m beed_benchmark --data-path data/BEED_Data.csv --tune-hyperparameters

# Run with feature selection
python -m beed_benchmark --data-path data/BEED_Data.csv --feature-selection

# Resume interrupted experiment
python -m beed_benchmark --data-path data/BEED_Data.csv --resume
```

### Jupyter Notebook

```python
# Load example notebook
from beed_benchmark.examples import load_example_notebook
load_example_notebook("01_basic_benchmark.ipynb")
```

## ğŸ“ Project Structure

```
beed_benchmark/
â”œâ”€â”€ ğŸ“‚ src/beed_benchmark/          # Main package
â”‚   â”œâ”€â”€ __init__.py                 # Package initialization
â”‚   â”œâ”€â”€ config.py                   # Configuration management
â”‚   â”œâ”€â”€ data_preprocessing.py       # Data loading & preprocessing
â”‚   â”œâ”€â”€ model_definitions.py        # ML model definitions
â”‚   â”œâ”€â”€ feature_selection.py        # Feature selection algorithms
â”‚   â”œâ”€â”€ hyperparameter_tuning.py    # Hyperparameter optimization
â”‚   â”œâ”€â”€ evaluation.py               # Model evaluation & metrics
â”‚   â”œâ”€â”€ external_validation.py      # Cross-dataset validation
â”‚   â”œâ”€â”€ visualization.py            # Plotting & visualization
â”‚   â””â”€â”€ utils.py                    # Utility functions
â”œâ”€â”€ ğŸ“‚ notebooks/                   # Jupyter notebooks
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb   # Exploratory data analysis
â”‚   â”œâ”€â”€ 02_benchmark_execution.ipynb # Main benchmark execution
â”‚   â”œâ”€â”€ 03_results_analysis.ipynb   # Results analysis
â”‚   â””â”€â”€ 04_external_validation.ipynb # External validation
â”œâ”€â”€ ğŸ“‚ data/                        # Data directory
â”‚   â”œâ”€â”€ raw/                        # Raw datasets
â”‚   â”œâ”€â”€ processed/                  # Processed datasets
â”‚   â””â”€â”€ external/                   # External validation datasets
â”œâ”€â”€ ğŸ“‚ results/                     # Results directory
â”‚   â”œâ”€â”€ models/                     # Trained models
â”‚   â”œâ”€â”€ metrics/                    # Evaluation metrics
â”‚   â”œâ”€â”€ figures/                    # Generated plots
â”‚   â””â”€â”€ reports/                    # Analysis reports
â”œâ”€â”€ ğŸ“‚ tests/                       # Unit tests
â”œâ”€â”€ ğŸ“‚ docs/                        # Documentation
â”œâ”€â”€ ğŸ“‚ examples/                    # Example scripts
â”œâ”€â”€ requirements.txt                # Dependencies
â”œâ”€â”€ requirements-dev.txt            # Development dependencies
â”œâ”€â”€ setup.py                        # Package setup
â”œâ”€â”€ pyproject.toml                  # Project configuration
â”œâ”€â”€ .github/                        # GitHub workflows
â””â”€â”€ README.md                       # This file
```

## ğŸ“– Usage Guide

### 1. Data Preparation

Your EEG dataset should be in CSV format with features in columns and the target variable in the last column:

```csv
feature_1,feature_2,...,feature_n,target
0.123,0.456,...,0.789,0
0.234,0.567,...,0.890,1
...
```

Classes should be encoded as:
- `0`: Non-seizure (normal/interictal)
- `1`: Seizure (ictal)
- `2`: Pre-seizure (preictal) - if applicable

### 2. Configuration

Customize your experiment settings:

```python
from beed_benchmark.config import CONFIG

# Experiment settings
CONFIG.CV_FOLDS = 10              # Cross-validation folds
CONFIG.RANDOM_STATE = 42          # Reproducibility seed
CONFIG.N_ITER_RANDOM_SEARCH = 100 # Hyperparameter search iterations

# Performance settings
CONFIG.N_JOBS = -1                # Parallel processing
CONFIG.MEMORY_LIMIT = "8GB"       # Memory limit per job
```

### 3. Model Selection

Choose specific models to benchmark:

```python
from beed_benchmark import BEEDBenchmark

benchmark = BEEDBenchmark(
    models=['Random Forest', 'XGBoost', 'SVM', 'Neural Network']
)
```

### 4. Advanced Features

#### Feature Selection

```python
from beed_benchmark.feature_selection import FeatureSelector

selector = FeatureSelector(
    method='rfe',           # Recursive Feature Elimination
    estimator='xgboost',    # Base estimator
    n_features=100          # Number of features to select
)

X_selected = selector.fit_transform(X, y)
```

#### Hyperparameter Tuning

```python
from beed_benchmark.hyperparameter_tuning import HyperparameterTuner

tuner = HyperparameterTuner(
    search_method='random',  # 'random', 'grid', 'bayesian'
    cv_folds=5,
    n_iter=50
)

best_params = tuner.optimize(model, X, y)
```

#### External Validation

```python
from beed_benchmark.external_validation import ExternalValidator

validator = ExternalValidator()
external_results = validator.validate(
    trained_models,
    external_dataset_path="data/external/test_data.csv"
)
```

## âš™ï¸ Configuration

The pipeline uses a hierarchical configuration system:

### Project Paths (`ProjectPaths`)
```python
BASE_PATH: Path          # Project root directory
DATA_DIR: Path          # Raw data location
PROCESSED_DIR: Path     # Processed data cache
RESULTS_DIR: Path       # Results output
MODELS_DIR: Path        # Saved models
FIGURES_DIR: Path       # Generated plots
```

### Experiment Settings (`ExperimentConfig`)
```python
RANDOM_STATE: int = 42                    # Reproducibility seed
CV_FOLDS: int = 10                       # Cross-validation folds
TEST_SIZE: float = 0.2                   # Train/test split ratio
N_ITER_RANDOM_SEARCH: int = 50          # Hyperparameter search iterations
MAX_SHAP_SAMPLES: int = 2000            # SHAP analysis sample limit
FEATURE_SELECTION_CV_FOLDS: int = 5     # Feature selection CV folds
FEATURE_SELECTION_RUNS: int = 3         # Feature selection stability runs
```

## ğŸ¤– Supported Models

| Model | Type | Hyperparameter Tuning | Feature Importance | Probability Output |
|-------|------|----------------------|-------------------|-------------------|
| **Logistic Regression** | Linear | âœ… | âœ… | âœ… |
| **K-Nearest Neighbors** | Instance-based | âœ… | âŒ | âœ… |
| **Support Vector Machine** | Kernel-based | âœ… | âŒ | âœ… |
| **Decision Tree** | Tree-based | âœ… | âœ… | âœ… |
| **Random Forest** | Ensemble | âœ… | âœ… | âœ… |
| **Gradient Boosting** | Ensemble | âœ… | âœ… | âœ… |
| **XGBoost** | Ensemble | âœ… | âœ… | âœ… |
| **LightGBM** | Ensemble | âœ… | âœ… | âœ… |
| **Multi-layer Perceptron** | Neural Network | âœ… | âŒ | âœ… |

### Adding Custom Models

```python
from beed_benchmark.model_definitions import ModelFactory

# Define custom model
class CustomModelFactory(ModelFactory):
    @staticmethod
    def get_custom_model():
        from sklearn.ensemble import ExtraTreesClassifier
        return ExtraTreesClassifier(random_state=42)
    
    @staticmethod
    def get_custom_hyperparameters():
        return {
            'n_estimators': randint(100, 500),
            'max_depth': [10, 20, None],
            'min_samples_split': randint(2, 11)
        }

# Register custom model
ModelFactory.register_model('Extra Trees', CustomModelFactory.get_custom_model())
```

## ğŸ“Š Results & Evaluation

### Evaluation Metrics

The benchmark evaluates models using comprehensive metrics:

**Classification Metrics:**
- Accuracy
- Precision (macro & weighted)
- Recall (macro & weighted) 
- F1-score (macro & weighted)
- ROC-AUC (one-vs-rest)

**Performance Metrics:**
- Training time
- Inference time
- Memory usage

**Statistical Analysis:**
- Mean Â± Standard deviation across CV folds
- Statistical significance testing
- Effect size analysis

### Results Format

```python
# Results DataFrame structure
results = pd.DataFrame({
    'Model': str,                    # Model name
    'accuracy_mean': float,          # Mean accuracy
    'accuracy_std': float,           # Accuracy standard deviation
    'f1_weighted_mean': float,       # Mean weighted F1-score
    'f1_weighted_std': float,        # F1-score standard deviation
    'train_time_mean': float,        # Mean training time (seconds)
    'inference_time_mean': float,    # Mean inference time (seconds)
    # ... additional metrics
})
```

### Visualization

The pipeline generates publication-ready visualizations:

- **Performance Comparison**: Horizontal bar charts with error bars
- **ROC Curves**: Multi-class ROC analysis
- **Confusion Matrices**: Detailed classification results
- **Feature Importance**: Top contributing features
- **Learning Curves**: Model convergence analysis
- **Statistical Tests**: Significance testing results

## ğŸ”¬ Example Results

Based on evaluation of EEG epilepsy detection datasets:

| Model | Weighted F1-Score | Accuracy | Training Time (s) | Inference Time (ms) |
|-------|------------------|----------|------------------|-------------------|
| **XGBoost** | 0.924 Â± 0.012 | 0.925 Â± 0.011 | 2.34 Â± 0.23 | 1.2 Â± 0.1 |
| **Random Forest** | 0.918 Â± 0.015 | 0.919 Â± 0.014 | 1.89 Â± 0.15 | 0.8 Â± 0.1 |
| **LightGBM** | 0.916 Â± 0.013 | 0.917 Â± 0.012 | 1.45 Â± 0.12 | 0.9 Â± 0.1 |
| **SVM** | 0.908 Â± 0.018 | 0.909 Â± 0.017 | 12.4 Â± 1.2 | 4.5 Â± 0.3 |
| **Neural Network** | 0.895 Â± 0.021 | 0.896 Â± 0.020 | 8.7 Â± 0.8 | 2.1 Â± 0.2 |

> Results may vary based on dataset characteristics and preprocessing methods.

## ğŸ“š Documentation

Comprehensive documentation is available:

- **[API Reference](./docs/api/)**: Complete API documentation
- **[User Guide](./docs/user_guide/)**: Step-by-step tutorials
- **[Examples](./examples/)**: Code examples and use cases
- **[Paper Supplements](./docs/paper/)**: Reproducible research materials

## ğŸ§ª Testing

Run the test suite:

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=beed_benchmark

# Run specific test category
pytest tests/test_models.py
pytest tests/test_evaluation.py
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ† Citation

If you use BEED Benchmark in your research, please cite our work:

```bibtex
@software{beed_benchmark2024,
  title={BEED Benchmark: A Modular Machine Learning Pipeline for EEG-based Epileptic Seizure Detection},
  author={Nandatama, Engki},
  year={2025},
  publisher={GitHub},
  url={https://github.com/engkinandatama/beed-benchmark},
  version={1.0.0}
}
```
<!--
### Related Publications

```bibtex
@article{your_paper2024,
  title={A Comprehensive Benchmark and Robustness Analysis of Machine Learning Models for Epileptic Seizure Classification on the BEED Dataset},
  author={Nandatama, Engki},
  journal={Journal Name},
  year={2025},
  doi={10.xxxx/xxxx}
}
```
-->
## ğŸ™ Acknowledgments

- **Contributors**: Thank you to all contributors who have helped improve this project
- **Data Providers**: Thanks to institutions providing EEG datasets for research
- **Open Source Community**: Built on excellent open-source libraries including scikit-learn, XGBoost, and pandas
- **Research Community**: Inspired by best practices from machine learning and epilepsy research communities

## ğŸ“ Support & Contact

- **Issues**: [GitHub Issues](https://github.com/engkinandatama/beed-benchmark/issues)
- **Discussions**: [GitHub Discussions](https://github.com/engkinandatama/beed-benchmark/discussions)

## ğŸ”„ Changelog

See [CHANGELOG.md](CHANGELOG.md) for detailed version history and updates.

---

<div align="center">

[â­ Star this project](https://github.com/engkinandatama/beed-benchmark) if you find it useful!

</div>
